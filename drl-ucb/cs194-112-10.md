# Lec 10: Optimal Control and Plannning

$$
\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}=\arg \max _{\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}} \sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \text { s.t. } \mathbf{a}_{t+1}=f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)
$$

### Cross-entropy method (CEM)

cross-entropy method with continuous-valued inputs:
1. sample $\mathbf{A}_{1}, \ldots, \mathbf{A}_{N}$ form $p(\mathbf{A})$
2. evaluate $J(\mathbf{A}_{1}), \ldots, J(\mathbf{A}_{N})$
3. pick the $\textit{elites}$ $\mathbf{A}_{i_1}, \ldots, \mathbf{A}_{i_M}$ with the highest value, where $M < N$
4. refit $p(\mathbf{A})$ to the elites $\mathbf{A}_{i_i}, \ldots, \mathbf{A}_{i_M}$

### Model-based RL

1. run base policy $\pi_0(\mathbf{a}_t | \mathbf{s}_t)$ (e.g., random policy) to collect $\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\}$ 
2. learn dynamics model $f(\mathbf{s}, \mathbf{a})$ to minimize 


### Variational Inference

#### Probabilistic latent variable models


#### Variational inference


#### Amortized variational inference

#### Variational autoencoders

