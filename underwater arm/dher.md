# DHER

1. Dynamic Goals

Dynamic goals are not static and change at every timestep. We follow the multi-goal setting. The goals are parts of the environment and do not influence the environment dynamics. We also assume that a dynamic goal $g_t\in{\Bbb G}$ moves by following some law $g_t=g(t;\gamma)$, where $\gamma$ parameterizeds the law (e.g., acceleration in Newton's law of motion), and yet its underlying moving law is unknown to the agent.

Moreover, we need to have some basic knowledge of goals, i.e., the measure of goal similarity on ${\Bbb G}$. We assume that $g_t\in\Bbb{G}$ corresponds to some predicate $f_{g_t}:S\rightarrow\{0,1\}$ and that the agent's goal is to achieve any state $s$ that satisfies $f_{g_t}=1$. We use $S=\Bbb{G}$ and define $f_{g_t}:=[s=g_t]$, which can be considered as a measure of goal similarity between $g_t$ and $s$. The goals can also specify only some properties of the state. Take manipulation tasks for instance: $\Bbb{G}=\Bbb{R^3}$ corresponds to the 3D positions of an object $s^{obj}$ about which the observation could include additional properties of the object. For a more concrete example, consider pushing a block towards a moving target position. The success of a task is defined as $f(s_t,g_t)=1_{condition}(||s_t^{obj}-g_t||\leq\epsilon)$, where $s_t^{obj}$ is the position of the object in the state $s_t$ and $\epsilon$ denotes a tolerance by the environment. $1_{condition}$ is an indicator function. The agent aims to achieve any state $s_t$ that satisfies $f(s_t,g_t)=1$. It receives a sparse reward $r_t:=r(s_t,a_t,g_{t+1})=-1_{condition}(f(s_{t+1},g_{t+1})\neq1)$ upon making an action $a_t$.

It is worth discussing the main difference between the implications of the dynamic goals and the static ones. Expressing a static goal in the following way, $g_t^{static}=g^{static},\forall{t}$, highlights the key challenge of dealing with the dynamic goal. Namely, the agent has no access to the underlying law of the dynamic goal in our setting, whereas the law of being static is known to the agent in. In other words, the agent has no clue at all how to construct a new dynamic goal that is admissible by environment.
