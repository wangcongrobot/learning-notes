# CS285 Deep Reinforcement Learning HW4: Model-Based RL

## 2 Model-Based Reinforcement Learning

We will now provide a brief overview of model-based reinforcement learning (MBRL), and the specific type of MBRL you will be implementing in this homework.

MBRL consists primarily of two aspects: (1) learning a dynamics model and (2) using the learned dynamics models to plan and execute actions that minimize a cost function (or maximize a reward function).

### 2.1 Dynamics Model

In this assignment, you will learn a neural network dynamics model of the form:

$$
\hat{\Delta}_{t+1}=f_\theta(\mathbf{s}_t,\mathbf{a}_t) \quad(1)
$$

such that 
$$
\mathbf{\hat{s}}_{t+1}=\mathbf{s}_t+\hat{\Delta}_{t+1} \quad (2)
$$
in which the neural network $f_\theta$ encodes the change in state that occurs as a result of executing the action $\mathbf{a}_t$ from state $\mathbf{s}_t$. See the previously referenced [paper](https://arxiv.org/pdf/1708.02596.pdf) for intuition on why we might want our network to predict state differences, instead of directly predicting next state.

You will train $f_\theta$ in a standard supervised learning setup, by performing gradient descent on the following objective:

$$
\mathcal{L}(\theta)=\sum_{(\mathbf{s}_t,\mathbf{a}_t,\mathbf{s}_{t+1})\in \mathcal{D}}||(\mathbf{s}_{t+1}-\mathbf{s}_t)-f_\theta(\mathbf{s}_t,\mathbf{a}_t)||_2^2 \quad(3)\\
=\sum_{(\mathbf{s}_t,\mathbf{a}_t,\mathbf{s}_{t+1})\in \mathcal{D}}||\Delta_{t+1}-\hat{\Delta}_{t+1}||_2^2 \quad (4)
$$

### 2.2 Action Selection

Given the learned dynamics model, we now want to select and execute actions that minimize a known cost function (or maximize a known reward function). Ideally, you would calculate these actions by solving the following optimization:
$$
\mathbf{a}_t^*=\arg\min_{\mathbf{a}_t:\infin}\sum_{t'=t}^\infin c(\mathbf{\hat{s}_{t'},\mathbf{a}_{t'}}) \quad
\text{s.t.} \quad \mathbf{\hat{s}}_{t'+1}=\mathbf{\hat{s}}_{t'}+f_\theta(\mathbf{\hat{s}}_{t'},\mathbf{a}_{t'}) \quad(5)
$$

However, solving Eqn 5 is impractical for two reasons: (1) planning over an infinite sequence of actions is impossible and (2) the learned dynamics model is imperfect, so using it to plan in such an open-loop manner will lead to accumulating errors over time and planning far into the future will become very inaccurate.

Instead, we will solve the following gradient-free optimization problem:
$$
\mathbf{A}^*=\arg \min_{{\mathbf{A}^{(0)},\dots,\mathbf{A}^{(K-1)}}}\sum_{t'=t}^{t+H-1}c(\mathbf{\hat{s}}_{t'},\mathbf{a}_{t'}) \quad \text{s.t.} \quad \mathbf{\hat{s}}_{t'+1}=f\theta(\mathbf{\hat{s}}_{t'},\mathbf{a}_{t'}) \quad (6)
$$
in which $\mathbf{A}^{(k)}$ are each a random action sequence of length $H$. What Eqn. 6 says is to consider $K$ random action sequences of length $H$, predict the result (i.e., future states) of taking each of these action sequences using the learned dynamics model $f_\theta$, evaluate the cost/reward associated with each candidate action sequence, and select the best action sequence. Note that this approach only plans $H$ steps into the future, which is desirable because if prevent accumalting model error, but is also not desirable because it may not be sufficient for solving long-horizon tasks.

Additionally, since our model is imperfect and things will never go perfectly according to plan, we adopt a model predictive control (MPC) approach, in which we solve Eqn. 6 at every time step to select the best $H$-step action sequence, but then we execute only the first action from that sequence before replanning again at the next time step using updated state information.

Finally, note that the random-shooting optimization approach mentioned above can be greatly improved. 


### 2.3 On-Policy Data Collection

Although MBRL is in theory off-policy -- meaning it can learn from any data -- in practice it will perform poorly if you don't have on-policy data. In other words, if a model is trained on only randomly-collected data, it will (in most cases) be insufficient to describe the parts of the state space that we may actually care about. We can therefore use on-policy data collection in an iteractive algorithm to improve overall task performance. This is summarized as follows:

## [CS285 Lecture11](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf)

1. Basics of model-based RL: learn a model, use model for control
- Why does naive approach not work?
- The effect of distributional shift in model-based RL
2. Uncertainty in model-based RL
3. Model-based RL with complex observations
4. Next time: **policy learning** with model-based RL
- Goals:
  - Understand how to build model-based RL algorithms
  - Understand the important considerations for model-based RL
  - Understand the tradeoffs between different model class choices

**model-based reinforcement learning version 0.5:**
1. run base policy $\pi_0(\mathbf{a}_t|\mathbf{s}_t)$ (e.g., random policy) to collect $\mathcal{D}=\{(\mathbf{s,a,s'})_i\}$ 
2. learn dynamics model $f(\mathbf{s,a})$ to minimize $\sum_i||f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}'_i||^2$
3. plan through $f(\mathbf{s,a})$ to choose actions

**model-based reinforcement learning version 1.0:**
1. run base policy $\pi_0(\mathbf{a}_t|\mathbf{s}_t)$ (e.g., random policy) to collect $\mathcal{D}=\{(\mathbf{s,a,s'})_i\}$ 
2. learn dynamics model $f(\mathbf{s,a})$ to minimize $\sum_i||f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}'_i||^2$
3. plan through $f(\mathbf{s,a})$ to choose actions
4. execute those actions and add the resulting data ${\mathbf{s,a,s'}}$ to $\mathcal{D}$

**model-based reinforcement learning version 1.5:**
1. run base policy $\pi_0(\mathbf{a}_t|\mathbf{s}_t)$ (e.g., random policy) to collect $\mathcal{D}=\{(\mathbf{s,a,s'})_i\}$ 
2. learn dynamics model $f(\mathbf{s,a})$ to minimize $\sum_i||f(\mathbf{s}_i,\mathbf{a}_i)-\mathbf{s}'_i||^2$
3. plan through $f(\mathbf{s,a})$ to choose actions
4. execute the first planned action, observe resulting state $\mathbf{s'}$ (MPC)
5. append $(\mathcal{s,a,s'})$ to dataset $\mathcal{D}$

[HW4 code analysis](https://github.com/xuanlinli17/CS285_Fa19_Deep_Reinforcement_Learning)

```python
# MPC_policy.py
import numpy as np
from .base_policy import BasePolicy

class MPCPolicy(BasePolicy):
    def __init__(self,
                 sess,
                 env,
                 ac_dim,
                 dyn_models,
                 horizon,
                 N,
                 **kwargs):
        
        # init vars
        self.sess = sess
        self.env = env
        self.dyn_models = dyn_models
        self.horizon = horizon
        self.N = N
        self.data_statistics = None # NOTE must be updated from elsewhere

        self.ob_dim = self.env.observation_space.shape[0]

        # action space
        self.ac_space = self.env.action_space
        self.ac_dim = ac_dim
        self.low = self.ac_space.low
        self.high = self.ac_space.high

    def sample_action_sequence(self, num_sequences, horizon):
        random_action_sequences = np.random.uniform(self.low, self.high, size=[num_sequences, horizon, self.ac_dim])
        return random_action_sequences

    def get_action(self, obs):
        if self.data_statistics is None:
            return self.sample_action_sequences(num_sequences=1, horizon=1)[0]

        # sample random actions (N x horizon)
        candidate_action_sequences = self.sample_action_sequences(num_sequences=self.N, horizon=self.horizon)

        # a list you can use for storing the predicted reward for each candidate sequence
        # shape = (#models, self.N)
        predicted_rewards_per_ens = []

        for model in self.dyn_models:

    

```

## CS294 HW4

```python
data = sample(env, random_controller, num_paths_random, env_horizon)

作者：糖葫芦喵喵
链接：https://zhuanlan.zhihu.com/p/37331693
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

for _ in tqdm.tqdm(range(num_paths)):# 循环num_paths次
	# 初始化环境和参数
        ob = env.reset()
        obs, next_obs, acs, rewards, costs = [], [], [], [], []
        steps = 0
	while True:
		obs.append(ob)
		ac = controller.get_action(ob) # 执行策略获得动作(其实是self.env.action_space.sample())
		acs.append(ac)
		ob, rew, done, _ = env.step(ac) # 执行该动作获得下一状态、回报值
		next_obs.append(ob)
		rewards.append(rew)
		steps += 1
		if done or steps >= horizon: #当episode结束或超过horizon步后结束这个episode
			break
	path = {"state": np.array(obs),
			"next_state": np.array(next_obs),
			"reward": np.array(rewards),
			"action": np.array(acs)}
	paths.append(path)

作者：糖葫芦喵喵
链接：https://zhuanlan.zhihu.com/p/37331693
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

normalization = compute_normalization(data)

# 上面计算好的normalization传进来
self.mean_s, self.std_s, self.mean_deltas, self.std_deltas, self.mean_a, self.std_a = normalization
self.sess = sess
self.batch_size = batch_size
self.iter = iterations
self.s_dim = env.observation_space.shape[0]
self.a_dim = env.action_space.shape[0]
# 状态-动作placeholder(None,s_dim,a_dim)以及(s_{t+1} - s_t)的placeholder(None,s_dim)
self.s_a = tf.placeholder(shape=[None, self.s_dim + self.a_dim], name="s_a", dtype=tf.float32)
self.deltas = tf.placeholder(shape=[None, self.s_dim], name="deltas", dtype=tf.float32)
# 输出预测
self.deltas_predict = build_mlp(self.s_a, self.s_dim, "NND", n_layers=n_layers, size=size,
						 activation=activation, output_activation=output_activation)
# loss
self.loss = tf.reduce_mean(tf.square(self.deltas_predict - self.deltas))
# AdamOptimizer
self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)

```