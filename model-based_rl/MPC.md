# Model Predictive Control (MPC)

### Planning and control with learned dynamics

This section describes different ways uncertainty can be incorporated into planning using probabilistic dynamics models. Once a model $\tilde{f_\theta}$ is learned, we can use it for control by predicting the future outcomes of candidate policies or actions and then selecting the particular candidate that is predicted to result in the highest reward. MBRL planning in discrete time over long time horizons is generally performed by using the dynamics model to recursively predict how an estimated Markov state will evolve from one time step to the next, e.g.: $s_{t+2}\sim \text{Pr}(s_{t+2}|s_{t+1},a_{t+1})$ where $s_{t+1} \sim \text{Pr}(s_{t+1}|s_t,a_t)$. When planning, we might consider each action $a_t$ to be a function of state, forming a policy $\pi:s_t \rightarrow a_t$, a function to optimize. Alternatively, we can plan and optimize for a sequence of actions, a process called model predictive control (MPC). We use MPC in our own experiments for several reasons, including implementation simplicity, lower computational burden (no gradients), and no requirement to specify the task-horizon in advance, whilst achieving the same data-efficiency as Gal who used a Bayesian NN with a policy to learn the cart-pole task in 2000 time steps. 